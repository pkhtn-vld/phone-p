<!doctype html>
<html>

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>webrtc-minimal</title>
  <style>
    body { font-family: Arial, Helvetica, sans-serif; padding: 16px }
    button { padding: 8px 12px; margin: 6px }
    #log { white-space: pre-wrap; background: #f6f6f6; padding: 8px; border-radius: 6px; height: 240px; overflow: auto }
  </style>
</head>

<body>
  <h3>WebRTC minimal demo</h3>
  <div>
    <button id="btnLocal">Разрешить микрофон</button>
    <button id="enableAudioBtn">Включить звук (нужен для авто-воспроизведения)</button>
    <button id="btnCall" disabled>Позвонить (всем)</button>
    <button id="btnHang">Завершить</button>
  </div>
  <div>Ваш id: <span id="myId">-</span></div>
  <div>Логи:</div>
  <div id="log"></div>

  <audio id="remoteAudio" autoplay playsinline></audio>

  <script>
    (function () {
      const logEl = document.getElementById('log');
      const myIdEl = document.getElementById('myId');
      const btnCall = document.getElementById('btnCall');
      function log(...args) { console.log(...args); logEl.textContent += args.map(a => typeof a === 'object' ? JSON.stringify(a) : String(a)).join(' ') + '\n'; logEl.scrollTop = logEl.scrollHeight; }

      let audioEnabled = false;
      const enableBtn = document.getElementById('enableAudioBtn');

      const remoteAudio = document.getElementById('remoteAudio');
      // Начально muted — чтобы браузер не пытался автопроигрывать без жеста.
      remoteAudio.muted = true;
      remoteAudio.autoplay = true;
      remoteAudio.setAttribute('playsinline', ''); // на будущее, если будете видео

      // AudioContext может понадобиться для визуализации/анализаторов.
      // Создаём лениво только при необходимости.
      window.remoteAudioContext = null;

      enableBtn.addEventListener('click', async () => {
        try {
          // Установим флаг и разблокируем аудио.
          audioEnabled = true;
          enableBtn.textContent = 'Звук включён';
          enableBtn.disabled = true;

          // Включаем кнопку "Позвонить" только после жеста пользователя.
          btnCall.disabled = false;

          // Resume AudioContext, если он был создан ранее (или создадим новый "разбудив" звук)
          try {
            if (!window.remoteAudioContext) {
              window.remoteAudioContext = new (window.AudioContext || window.webkitAudioContext)();
            }
            await window.remoteAudioContext.resume();
            log('// AudioContext resume OK');
          } catch (e) {
            log('// AudioContext resume failed', String(e));
          }

          // Если remote stream уже пришёл — назначим и запустим play() внутри обработчика клика (это важно для iOS)
          if (remoteAudio.srcObject) {
            try {
              remoteAudio.muted = false;
              await remoteAudio.play();
              log('// remote audio play OK (from enable click)');
            } catch (err) {
              log('// remoteAudio.play() failed (from enable click):', String(err));
            }
          }

        } catch (e) {
          log('// audio enable failed:', String(e));
        }
      });

      const wsProto = (location.protocol === 'https:') ? 'wss' : 'ws';
      const ws = new WebSocket(wsProto + '://' + location.host + '/ws');
      let myId = null;
      let pc = null;
      let localStream = null;

      function makeCallId() {
        try { if (myId) return `${myId}-${Date.now()}`; } catch (e) { }
        return `c-${Date.now()}-${Math.random().toString(36).slice(2, 6)}`;
      }

      const iceConfig = {
        iceServers: [
          { urls: 'stun:stun.l.google.com:19302' }
        ]
      };

      ws.addEventListener('open', () => log('// ws OPEN'));
      ws.addEventListener('message', async (ev) => {
        let msg; try { msg = JSON.parse(ev.data); } catch (e) { return; }
        log('// ws IN', msg.type || msg);

        if (msg.type === 'welcome') { myId = msg.id; myIdEl.textContent = myId; return; }

        if (msg.type === 'offer') {
          log('// got offer');
          if (pc) { pc.close(); pc = null; }
          const callId = msg.callId || makeCallId();
          pc = new RTCPeerConnection(iceConfig);

          pc.onicecandidate = (e) => { if (e.candidate) ws.send(JSON.stringify({ type: 'candidate', candidate: e.candidate, callId })); };

          pc.ontrack = (e) => {
            try {
              log('// ontrack', e.streams);
              const remoteStream = (e.streams && e.streams[0]) || e.stream || null;
              const aud = remoteAudio;
              aud.srcObject = remoteStream;

              // Если пользователь уже сделал жест — можно сразу play()
              if (audioEnabled) {
                aud.muted = false;
                // play() вызывается только когда был user gesture (enableBtn)
                aud.play().then(() => { log('// audio play OK (ontrack)'); })
                  .catch((err) => { log('// audio play failed (ontrack)', String(err)); });
              } else {
                // Оставляем muted и ждём нажатия enableAudioBtn
                aud.muted = true;
                log('// ontrack: waiting for user gesture to play');
              }
            } catch (e) {
              log('// ontrack handler failed', e);
            }
          };

          if (localStream) localStream.getTracks().forEach(t => pc.addTrack(t, localStream));
          await pc.setRemoteDescription({ type: 'offer', sdp: msg.sdp });
          const ans = await pc.createAnswer();
          await pc.setLocalDescription(ans);
          ws.send(JSON.stringify({ type: 'answer', sdp: ans.sdp, callId }));

          log('// answer sent');
          return;
        }

        if (msg.type === 'answer') {
          log('// got answer');
          if (!pc) return;
          try { await pc.setRemoteDescription({ type: 'answer', sdp: msg.sdp }); log('// remoteDescription(answer) set'); } catch (e) { log('// setRemoteDescription(answer) failed', e); }
          return;
        }

        if (msg.type === 'candidate') {
          log('// got candidate', msg.callId || '');
          if (!pc) { log('// no pc yet — ignored candidate'); return; }
          try { await pc.addIceCandidate(msg.candidate); log('// added candidate'); } catch (e) { log('// addIceCandidate failed', e); }
          return;
        }
      });

      document.getElementById('btnLocal').addEventListener('click', async () => {
        try {
          localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          log('// local stream acquired', localStream.getTracks().map(t => t.kind));
        } catch (e) {
          log('// getUserMedia failed', e);
        }
      });

      // Позвонить — запрещаем, если пользователь ещё не сделал жест для воспроизведения
      btnCall.addEventListener('click', async () => {
        log('// call: start');
        if (!audioEnabled) {
          alert('Пожалуйста, нажмите «Включить звук» перед звонком (требование iOS/Safari).');
          return;
        }

        if (pc) { pc.close(); pc = null; }
        const callId = makeCallId();

        pc = new RTCPeerConnection(iceConfig);
        window.pc = pc;
        pc.onicecandidate = (e) => { if (e.candidate) ws.send(JSON.stringify({ type: 'candidate', candidate: e.candidate, callId })); };
        pc.ontrack = (e) => {
          try {
            log('// ontrack', e.streams);
            const remoteStream = (e.streams && e.streams[0]) || e.stream || null;
            const aud = remoteAudio;
            aud.srcObject = remoteStream;

            if (audioEnabled) {
              aud.muted = false;
              aud.play().then(() => { log('// audio play OK'); })
                .catch((err) => { log('// audio play failed', String(err)); });
            } else {
              aud.muted = true;
              log('// ontrack: waiting for user gesture to play');
            }

          } catch (e) {
            log('// ontrack handler failed', e);
          }
        };
        if (localStream) localStream.getTracks().forEach(t => pc.addTrack(t, localStream));

        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);
        ws.send(JSON.stringify({ type: 'offer', sdp: offer.sdp }));
        log('// offer sent');
      });

      document.getElementById('btnHang').addEventListener('click', () => {
        if (pc) { pc.close(); pc = null; log('// pc closed'); }

        try {
          if (window._localAnalyserInterval) { clearInterval(window._localAnalyserInterval); window._localAnalyserInterval = null; }
          if (window.localAudioContext) { try { window.localAudioContext.close(); } catch (e) { } window.localAudioContext = null; }
        } catch (e) { }

        try {
          if (window._remoteAnalyserInterval) { clearInterval(window._remoteAnalyserInterval); window._remoteAnalyserInterval = null; }
          if (window.remoteAudioContext) { try { window.remoteAudioContext.close(); } catch (e) { } window.remoteAudioContext = null; }
        } catch (e) { }

        try {
          if (localStream) {
            localStream.getTracks().forEach(t => { try { t.stop(); } catch (e) { } });
            localStream = null;
            log('// local stream stopped');
          }
        } catch (e) { }
      });
    })();
  </script>
</body>

</html>
